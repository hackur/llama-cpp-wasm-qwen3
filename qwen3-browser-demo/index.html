<!DOCTYPE html>
<!--
  Qwen3 Browser Demo

  A client-side demonstration of running the Qwen3-0.6B language model
  directly in the browser using WebAssembly. This implementation uses
  the wllama library, which provides JavaScript bindings for llama.cpp.

  Requirements:
  - Modern browser with WebAssembly support
  - SharedArrayBuffer support (requires COOP/COEP headers)
  - Approximately 806MB of available memory for the model

  Architecture:
  - The wllama library handles WASM module loading and execution
  - Model inference runs in a Web Worker to avoid blocking the UI
  - Automatic fallback from multi-thread to single-thread mode

  References:
  - wllama: https://github.com/ngxson/wllama
  - llama.cpp: https://github.com/ggerganov/llama.cpp
  - Qwen3 models: https://huggingface.co/Qwen

  License: MIT
-->
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Qwen3 Browser Demo - wllama</title>

  <!--
    Styles for the demo interface.
    Uses system fonts for consistent cross-platform appearance.
    Color scheme follows accessibility guidelines for contrast ratios.
  -->
  <style>
    /* Reset and base styles */
    * {
      box-sizing: border-box;
    }

    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
      background-color: #f4f4f9;
      color: #333;
    }

    h1 {
      color: #2c3e50;
      text-align: center;
      margin-bottom: 10px;
    }

    .subtitle {
      text-align: center;
      color: #7f8c8d;
      margin-bottom: 20px;
    }

    /* Status bar showing model state */
    .status-bar {
      background-color: #fff;
      border: 1px solid #ddd;
      border-radius: 4px;
      padding: 10px 15px;
      margin-bottom: 20px;
      display: flex;
      justify-content: space-between;
      align-items: center;
    }

    .status-indicator {
      display: flex;
      align-items: center;
      gap: 8px;
    }

    /* Status indicator dot with color states */
    .status-dot {
      width: 10px;
      height: 10px;
      border-radius: 50%;
      background-color: #e74c3c; /* Red: not loaded */
    }

    .status-dot.loading {
      background-color: #f39c12; /* Orange: loading */
      animation: pulse 1s infinite;
    }

    .status-dot.ready {
      background-color: #27ae60; /* Green: ready */
    }

    @keyframes pulse {
      0%, 100% { opacity: 1; }
      50% { opacity: 0.5; }
    }

    /* Progress bar for model download */
    .progress-container {
      width: 100%;
      background-color: #e0e0e0;
      border-radius: 4px;
      margin: 10px 0;
      display: none;
    }

    .progress-bar {
      width: 0%;
      height: 20px;
      background-color: #3498db;
      border-radius: 4px;
      text-align: center;
      line-height: 20px;
      color: white;
      font-size: 12px;
      transition: width 0.3s ease;
    }

    /* Chat interface container */
    .chat-container {
      background-color: #fff;
      border: 1px solid #ddd;
      border-radius: 4px;
      padding: 15px;
      margin-bottom: 20px;
    }

    /* Scrollable response area */
    .response-area {
      min-height: 300px;
      max-height: 400px;
      overflow-y: auto;
      border: 1px solid #eee;
      border-radius: 4px;
      padding: 10px;
      margin-bottom: 15px;
      background-color: #fafafa;
    }

    .response-area p {
      margin: 0 0 10px 0;
      line-height: 1.5;
      word-wrap: break-word;
    }

    /* Message styling by type */
    .user-prompt {
      color: #2980b9;
      font-weight: bold;
    }

    .model-response {
      color: #2c3e50;
      white-space: pre-wrap;
    }

    .status-message {
      color: #7f8c8d;
      font-style: italic;
    }

    .error-message {
      color: #e74c3c;
    }

    /* Input area layout */
    .input-area {
      display: flex;
      gap: 10px;
    }

    textarea {
      flex: 1;
      padding: 10px;
      border: 1px solid #ccc;
      border-radius: 4px;
      resize: vertical;
      min-height: 60px;
      font-size: 14px;
      font-family: inherit;
    }

    textarea:focus {
      outline: 2px solid #3498db;
      outline-offset: 2px;
      border-color: #3498db;
    }

    /* Button styles */
    button {
      background-color: #3498db;
      color: white;
      border: none;
      padding: 10px 20px;
      border-radius: 4px;
      cursor: pointer;
      font-size: 14px;
      transition: background-color 0.3s ease;
    }

    button:hover:not(:disabled) {
      background-color: #2980b9;
    }

    button:active:not(:disabled) {
      background-color: #2471a3;
    }

    button:disabled {
      background-color: #95a5a6;
      cursor: not-allowed;
      opacity: 0.7;
    }

    /* Loading spinner for buttons */
    button.loading {
      position: relative;
      color: transparent;
    }

    button.loading::after {
      content: '';
      position: absolute;
      width: 16px;
      height: 16px;
      top: 50%;
      left: 50%;
      margin-left: -8px;
      margin-top: -8px;
      border: 2px solid #fff;
      border-radius: 50%;
      border-top-color: transparent;
      animation: spinner 0.6s linear infinite;
    }

    @keyframes spinner {
      to { transform: rotate(360deg); }
    }

    .button-row {
      display: flex;
      gap: 10px;
      margin-bottom: 15px;
    }

    /* Information panel */
    .info-panel {
      background-color: #fff;
      border: 1px solid #ddd;
      border-radius: 4px;
      padding: 15px;
      font-size: 13px;
      color: #555;
    }

    .info-panel h3 {
      margin: 0 0 10px 0;
      color: #2c3e50;
    }

    .info-panel ul {
      margin: 0;
      padding-left: 20px;
    }

    .info-panel li {
      margin-bottom: 5px;
    }
  </style>
</head>
<body>
  <h1>Qwen3 Browser Demo</h1>
  <p class="subtitle">Running Qwen3-0.6B locally in your browser with wllama</p>

  <!-- Status bar displays current model state -->
  <div class="status-bar">
    <div class="status-indicator">
      <div id="statusDot" class="status-dot"></div>
      <span id="statusText">Model not loaded</span>
    </div>
    <span id="modelInfo">Qwen3-0.6B-UD-Q8_K_XL (~806MB)</span>
  </div>

  <!-- Progress bar shown during model download -->
  <div class="progress-container" id="progressContainer">
    <div class="progress-bar" id="progressBar">0%</div>
  </div>

  <!-- Model control buttons -->
  <div class="button-row">
    <button id="loadModelBtn">Load Model</button>
  </div>

  <!-- Chat interface -->
  <div class="chat-container">
    <div id="responseArea" class="response-area">
      <p class="status-message">Click "Load Model" to start. The model (~806MB) will be downloaded and cached.</p>
    </div>
    <div class="input-area">
      <textarea id="promptInput" placeholder="Enter your prompt here..." rows="3"></textarea>
      <button id="sendBtn" disabled>Send</button>
    </div>
  </div>

  <!-- Information panel -->
  <div class="info-panel">
    <h3>About this demo</h3>
    <ul>
      <li>Runs entirely in your browser - no server required for inference</li>
      <li>Model is cached in IndexedDB after first download</li>
      <li>Uses WebAssembly SIMD with multi-threading when available</li>
      <li>Requires Cross-Origin-Embedder-Policy and Cross-Origin-Opener-Policy headers</li>
      <li>Powered by <a href="https://github.com/ngxson/wllama">wllama</a></li>
    </ul>
  </div>

  <!--
    Application JavaScript

    This module initializes the wllama library, handles model loading,
    and manages the chat interface for text generation.
  -->
  <script type="module">
    /**
     * @fileoverview Browser-based LLM inference using wllama
     *
     * This script provides the client-side logic for loading and running
     * the Qwen3-0.6B model using WebAssembly. The wllama library handles
     * the low-level WASM operations and provides a high-level API for
     * model loading and text generation.
     *
     * @see https://github.com/ngxson/wllama
     */

    import { Wllama } from './wllama/index.js';

    /**
     * Configuration paths for wllama WASM binaries.
     * The library automatically selects between single-thread and multi-thread
     * builds based on browser SharedArrayBuffer support.
     *
     * @const {Object.<string, string>}
     */
    const CONFIG_PATHS = {
      'single-thread/wllama.wasm': './wllama/single-thread/wllama.wasm',
      'multi-thread/wllama.wasm': './wllama/multi-thread/wllama.wasm',
    };

    /**
     * DOM element references.
     * Cached for performance to avoid repeated querySelector calls.
     */
    const loadModelBtn = document.getElementById('loadModelBtn');
    const sendBtn = document.getElementById('sendBtn');
    const promptInput = document.getElementById('promptInput');
    const responseArea = document.getElementById('responseArea');
    const statusDot = document.getElementById('statusDot');
    const statusText = document.getElementById('statusText');
    const progressContainer = document.getElementById('progressContainer');
    const progressBar = document.getElementById('progressBar');

    /**
     * Application state.
     * @type {Wllama|null} wllama - The wllama instance after initialization
     * @type {boolean} isModelLoaded - Whether the model is ready for inference
     */
    let wllama = null;
    let isModelLoaded = false;

    /**
     * Appends a message to the response area.
     *
     * @param {string} text - The message text to display
     * @param {string} [className=''] - CSS class for styling the message
     * @returns {HTMLParagraphElement} The created paragraph element
     */
    function addMessage(text, className = '') {
      const p = document.createElement('p');
      p.textContent = text;
      if (className) p.className = className;
      responseArea.appendChild(p);
      responseArea.scrollTop = responseArea.scrollHeight;
      return p;
    }

    /**
     * Clears all messages from the response area.
     */
    function clearResponses() {
      responseArea.innerHTML = '';
    }

    /**
     * Updates the status indicator display.
     *
     * @param {string} status - Status type: 'loading', 'ready', or 'error'
     * @param {string} text - Status message to display
     */
    function setStatus(status, text) {
      statusDot.className = 'status-dot';
      if (status === 'loading') {
        statusDot.classList.add('loading');
      } else if (status === 'ready') {
        statusDot.classList.add('ready');
      }
      statusText.textContent = text;
    }

    /**
     * Updates the progress bar display.
     *
     * @param {number} percent - Progress percentage (0-100)
     */
    function setProgress(percent) {
      progressBar.style.width = percent + '%';
      progressBar.textContent = percent + '%';
    }

    /**
     * Handles the model loading process.
     *
     * This function initializes the wllama instance and loads the GGUF model
     * file. The model is downloaded from the server and cached in IndexedDB
     * for subsequent loads.
     *
     * @listens click
     * @async
     */
    loadModelBtn.addEventListener('click', async () => {
      if (isModelLoaded) {
        addMessage('Model is already loaded!', 'status-message');
        return;
      }

      clearResponses();
      addMessage('Loading model... This may take a while on first load (~806MB download)', 'status-message');
      setStatus('loading', 'Loading model...');
      progressContainer.style.display = 'block';

      loadModelBtn.disabled = true;
      loadModelBtn.classList.add('loading');

      try {
        /*
         * Initialize wllama with configuration.
         * allowOffline enables loading from IndexedDB cache when available.
         */
        wllama = new Wllama(CONFIG_PATHS, {
          allowOffline: true,
        });

        /*
         * Construct absolute URL for the model file.
         * This is required because wllama workers need absolute URLs.
         */
        const modelUrl = new URL('./models/Qwen3-0.6B-UD-Q8_K_XL.gguf', window.location.href).href;

        /*
         * Load the model with progress tracking.
         * n_ctx sets the context window size for inference.
         */
        await wllama.loadModelFromUrl(modelUrl, {
          n_ctx: 2048,
          progressCallback: ({ loaded, total }) => {
            const percent = Math.round((loaded / total) * 100);
            setProgress(percent);
            setStatus('loading', `Loading model... ${percent}%`);
          },
        });

        isModelLoaded = true;
        progressContainer.style.display = 'none';
        setStatus('ready', 'Model loaded and ready!');
        loadModelBtn.disabled = false;
        loadModelBtn.classList.remove('loading');
        loadModelBtn.textContent = 'Model Loaded';
        sendBtn.disabled = false;
        addMessage('Model loaded successfully! You can now send prompts.', 'status-message');

      } catch (error) {
        console.error('Failed to load model:', error);
        addMessage('Failed to load model: ' + error.message, 'error-message');
        setStatus('error', 'Failed to load');
        progressContainer.style.display = 'none';
        loadModelBtn.disabled = false;
        loadModelBtn.classList.remove('loading');
      }
    });

    /**
     * Handles prompt submission and text generation.
     *
     * This function sends the user's prompt to the model and streams
     * the response tokens to the UI as they are generated.
     *
     * @listens click
     * @async
     */
    sendBtn.addEventListener('click', async () => {
      const prompt = promptInput.value.trim();
      if (!prompt) return;
      if (!isModelLoaded || !wllama) {
        addMessage('Please load the model first!', 'error-message');
        return;
      }

      addMessage('You: ' + prompt, 'user-prompt');
      promptInput.value = '';
      promptInput.disabled = true;
      sendBtn.disabled = true;
      sendBtn.classList.add('loading');

      /* Create placeholder element for streaming response */
      const responseEl = addMessage('Model: ', 'model-response');
      let currentText = '';

      try {
        /*
         * Generate completion with streaming.
         *
         * Sampling parameters:
         * - nPredict: Maximum number of tokens to generate
         * - temp: Temperature for randomness (0.0 = deterministic, 1.0 = creative)
         * - top_k: Limits vocabulary to top K most likely tokens
         * - top_p: Nucleus sampling threshold
         */
        await wllama.createCompletion(prompt, {
          nPredict: 256,
          sampling: {
            temp: 0.7,
            top_k: 40,
            top_p: 0.9,
          },
          /**
           * Callback invoked for each generated token.
           *
           * @param {number} token - Token ID
           * @param {Uint8Array} piece - Token bytes
           * @param {string} fullText - Accumulated text so far
           */
          onNewToken: (token, piece, fullText) => {
            currentText = fullText;
            responseEl.textContent = 'Model: ' + currentText;
            responseArea.scrollTop = responseArea.scrollHeight;
          },
        });
      } catch (error) {
        console.error('Generation error:', error);
        addMessage('Error: ' + error.message, 'error-message');
      }

      sendBtn.disabled = false;
      sendBtn.classList.remove('loading');
      promptInput.disabled = false;
    });

    /**
     * Keyboard shortcut handler for the prompt input.
     * Allows submitting with Enter key (Shift+Enter for newline).
     *
     * @listens keydown
     * @param {KeyboardEvent} e - The keyboard event
     */
    promptInput.addEventListener('keydown', (e) => {
      if (e.key === 'Enter' && !e.shiftKey) {
        e.preventDefault();
        sendBtn.click();
      }
    });
  </script>
</body>
</html>
