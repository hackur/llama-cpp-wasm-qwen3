{
  "manifest_version": 3,
  "name": "llm-proxy-ext",
  "version": "0.1.0",
  "description": "A Chrome extension to proxy LLM requests, initially using a local Qwen3 model via llama-cpp-wasm.",
  "icons": {
    "16": "icons/ic_forum_24px.svg",
    "48": "icons/ic_forum_24px.svg",
    "128": "icons/ic_forum_24px.svg"
  },
  "action": {
    "default_popup": "html/popup.html",
    "default_icon": {
      "16": "icons/ic_forum_24px.svg",
      "48": "icons/ic_forum_24px.svg"
    }
  },
  "background": {
    "service_worker": "js/background.js",
    "type": "module"
  },
  "permissions": [
    "storage",
    "offscreen"
  ],
  "web_accessible_resources": [
    {
      "resources": [
        "icons/ic_forum_24px.svg",
        "js/lib/main.wasm",
        "models/*"
      ],
      "matches": [ "<all_urls>" ]
    }
  ]
}
