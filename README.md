# Qwen3 0.6B with llama-cpp-wasm

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A web-based implementation of the Qwen3 0.6B language model using `llama-cpp-wasm`, enabling in-browser execution with tool calling and code suggestion capabilities.

## ğŸš€ Features

- **In-Browser Execution**: Runs entirely in the browser using WebAssembly
- **Multi-threaded Processing**: Utilizes Web Workers for improved performance
- **Tool Calling**: Supports function calling capabilities
- **Code Suggestions**: Provides intelligent code completions for HTML, CSS, and JavaScript
- **No Server Required**: All processing happens client-side

## ğŸ“¦ Prerequisites

- Modern web browser (Chrome, Firefox, Edge, or Safari)
- Node.js (for local development)
- Git (for cloning the repository)

## ğŸ› ï¸ Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/llama-cpp-wasm-qwen3.git
   cd llama-cpp-wasm-qwen3
   ```

2. Install dependencies (for development):
   ```bash
   npm install -g http-server
   ```

3. Download the required WebAssembly assets:
   ```bash
   chmod +x scripts/download_llama_cpp_wasm_assets.sh
   ./scripts/download_llama_cpp_wasm_assets.sh
   ```

## ğŸƒâ€â™‚ï¸ Quick Start

1. Place your Qwen3 GGUF model in the models directory:
   ```bash
   mkdir -p qwen3-browser-demo/models
   # Copy your Qwen3-0.6B-UD-Q8_K_XL.gguf to qwen3-browser-demo/models/
   ```

2. Start a local web server:
   ```bash
   cd qwen3-browser-demo
   http-server -p 8080
   ```

3. Open your browser and navigate to:
   ```
   http://localhost:8080
   ```

## ğŸ§© Project Structure

```
qwen3-browser-demo/
â”œâ”€â”€ llama-mt/               # llama-cpp-wasm runtime files
â”‚   â”œâ”€â”€ llama.js            # Main library interface
â”‚   â”œâ”€â”€ main-worker.js      # Web Worker implementation
â”‚   â”œâ”€â”€ actions.js          # Action definitions
â”‚   â”œâ”€â”€ utility.js          # Helper functions
â”‚   â”œâ”€â”€ main.js             # WASM module loader
â”‚   â””â”€â”€ main.wasm           # Compiled WebAssembly module
â”œâ”€â”€ models/                 # Store your GGUF models here
â”‚   â””â”€â”€ model.bin           # Symlink to your actual model file
â””â”€â”€ index.html              # Demo interface

scripts/
â”œâ”€â”€ download_llama_cpp_wasm_assets.sh  # Asset downloader
â””â”€â”€ example_prd.txt                    # Example project requirements
```

## ğŸ¤– Usage

### Initialization

```javascript
import { LlamaCpp } from './llama-mt/llama.js';

// Initialize the model
const llama = new LlamaCpp(
  '/models/your-model.gguf',  // Path to GGUF model
  onModelLoaded,             // Callback when model is loaded
  onTokenGenerated,          // Callback for each generated token
  onGenerationComplete       // Callback when generation completes
);

function onModelLoaded() {
  console.log('Model loaded and ready');
}

function onTokenGenerated(token) {
  process.stdout.write(token);
}

function onGenerationComplete() {
  console.log('\nGeneration complete');
}
```

### Text Generation

```javascript
// Start text generation
llama.generate({
  prompt: 'Hello, world!',
  n_predict: 100,           // Number of tokens to generate
  temp: 0.7,                // Temperature (0-2, lower = more focused)
  top_k: 40,               // Top-k sampling
  top_p: 0.95,              // Nucleus sampling
  n_gpu_layers: 20          // Number of layers to offload to GPU
});

// Stop generation
llama.stop();
```

## ğŸ› ï¸ Development

### Building from Source

If you need to modify the WebAssembly components:

1. Install Emscripten SDK:
   ```bash
   git clone https://github.com/emscripten-core/emsdk.git
   cd emsdk
   ./emsdk install latest
   ./emsdk activate latest
   source ./emsdk_env.sh
   ```

2. Build the project:
   ```bash
   ./build-multi-thread.sh
   ```

### Adding Tool Calling

To implement tool calling functionality:

1. Define your tool specifications in `llama-mt/actions.js`
2. Update the worker message handler in `llama-mt/main-worker.js`
3. Implement tool execution logic in your main application

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [llama.cpp](https://github.com/ggerganov/llama.cpp) - C/C++ inference of LLaMA models
- [llama-cpp-wasm](https://github.com/tangledgroup/llama-cpp-wasm) - WebAssembly port of llama.cpp
- [Qwen](https://huggingface.co/Qwen) - The Qwen language models

## ğŸ“„ TODO

- [ ] Add more comprehensive error handling
- [ ] Implement streaming responses
- [ ] Add support for more model formats
- [ ] Improve documentation and examples
- [ ] Add CI/CD pipeline for automated testing

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.
